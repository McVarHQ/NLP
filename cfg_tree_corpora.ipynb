{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, FreqDist, bigrams\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.chunk.regexp import RegexpParser\n",
        "from nltk.tree import Tree\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "txBhsjruSjZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "d3zUesg9Putg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71eaf15-114f-43ab-c409-e5d2b533fd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au95QtYOO4QQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c7af4c-9960-4c60-b0af-75c6d20afeab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of sentences: 3\n",
            "Enter a sentence: Hi I am Navya\n",
            "Enter a sentence: I am pleased to meet you.\n",
            "Enter a sentence: Hope you are doing well.\n"
          ]
        }
      ],
      "source": [
        "def get_input_sentences():\n",
        "    sentences = []\n",
        "    num_sentences = int(input(\"Enter the number of sentences: \"))\n",
        "    for _ in range(num_sentences):\n",
        "        sentence = input(\"Enter a sentence: \")\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "# Get sentences from the user\n",
        "sentences = get_input_sentences()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "pos_tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "\n"
      ],
      "metadata": {
        "id": "ZuiRt23tXG9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS Tagging"
      ],
      "metadata": {
        "id": "7fZGSLg5T8UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tagged Sentences:\")\n",
        "for sentence in pos_tagged_sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# Define chunking grammar\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}   # Chunk sequences of DT, JJ, NN\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
        "    PP: {<IN><NP>}       # Chunk prepositions followed by NP\n",
        "    CLAUSE: {<NP><VP>}   # Chunk NP, VP\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pupLy1q0XPyP",
        "outputId": "391812c8-1a17-4aae-a264-dac13ec5b2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagged Sentences:\n",
            "[('Hi', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Navya', 'RB')]\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('pleased', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PRP'), ('.', '.')]\n",
            "[('Hope', 'NN'), ('you', 'PRP'), ('are', 'VBP'), ('doing', 'VBG'), ('well', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chunking"
      ],
      "metadata": {
        "id": "O8GUwmIIXHjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = RegexpParser(grammar)\n",
        "chunked_sentences = [chunk_parser.parse(sentence) for sentence in pos_tagged_sentences]\n",
        "\n",
        "# Display chunked sentences\n",
        "print(\"\\nChunked Sentences:\")\n",
        "for chunked_sentence in chunked_sentences:\n",
        "    print(chunked_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnco0UCpXE9e",
        "outputId": "6638bca4-ac03-4894-c9d5-0e020973fa7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunked Sentences:\n",
            "(S (NP Hi/NNP) I/PRP am/VBP Navya/RB)\n",
            "(S I/PRP am/VBP (NP pleased/JJ) to/TO meet/VB you/PRP ./.)\n",
            "(S (NP Hope/NN) you/PRP are/VBP doing/VBG well/RB ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CFG Tree"
      ],
      "metadata": {
        "id": "7ubnnmSEXLtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_trees = [Tree.fromstring(str(chunked_sentence)) for chunked_sentence in chunked_sentences]\n",
        "\n",
        "# Display CFG trees\n",
        "print(\"\\nCFG Trees:\")\n",
        "for tree in cfg_trees:\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrJe4IKmXFVQ",
        "outputId": "c8aca8ff-d60f-4095-815c-b88bfad2d1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CFG Trees:\n",
            "        S                   \n",
            "   _____|_______________     \n",
            "  |     |       |       NP  \n",
            "  |     |       |       |    \n",
            "I/PRP am/VBP Navya/RB Hi/NNP\n",
            "\n",
            "                      S                          \n",
            "   ___________________|____________________       \n",
            "  |     |      |      |       |     |      NP    \n",
            "  |     |      |      |       |     |      |      \n",
            "I/PRP am/VBP to/TO meet/VB you/PRP ./. pleased/JJ\n",
            "\n",
            "                    S                        \n",
            "    ________________|____________________     \n",
            "   |       |        |        |     |     NP  \n",
            "   |       |        |        |     |     |    \n",
            "you/PRP are/VBP doing/VBG well/RB ./. Hope/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Term frequency"
      ],
      "metadata": {
        "id": "P7lzAUSCXgWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "freq_dist = FreqDist(all_words)\n",
        "\n",
        "# Display frequency of each term\n",
        "print(\"\\nTerm Frequency:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vnmtRkhXZ5_",
        "outputId": "de6fd669-e02c-490a-e3f1-3fb9a3a91b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Term Frequency:\n",
            "Hi: 1\n",
            "I: 2\n",
            "am: 2\n",
            "Navya: 1\n",
            "pleased: 1\n",
            "to: 1\n",
            "meet: 1\n",
            "you: 2\n",
            ".: 2\n",
            "Hope: 1\n",
            "are: 1\n",
            "doing: 1\n",
            "well: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Highest frequency"
      ],
      "metadata": {
        "id": "vKhh6aDUYJ0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_word = freq_dist.most_common(1)[0][0]\n",
        "print(f\"\\nWord with Highest Frequency: {most_common_word} ({freq_dist[most_common_word]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKHuaa1eYMKO",
        "outputId": "511d1cd4-0003-47e7-d0c6-4416eb5ff1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word with Highest Frequency: I (2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Highly occuring bigram"
      ],
      "metadata": {
        "id": "ft6oChArYsTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_bigrams = list(bigrams(all_words))\n",
        "bigram_freq = FreqDist(all_bigrams)\n",
        "\n",
        "print(\"\\nHighly Occurring Bigrams:\")\n",
        "for bigram, freq in bigram_freq.items():\n",
        "    print(f\"{bigram}: {freq}\")\n",
        "\n",
        "# Find and display the bigram with the highest frequency\n",
        "highest_freq_bigram = max(bigram_freq, key=bigram_freq.get)\n",
        "print(f\"\\nBigram with Highest Frequency: {highest_freq_bigram} ({bigram_freq[highest_freq_bigram]})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42Nv0xUhYbBS",
        "outputId": "2c1b0139-7b69-455a-9215-fc25ff500d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Highly Occurring Bigrams:\n",
            "('Hi', 'I'): 1\n",
            "('I', 'am'): 2\n",
            "('am', 'Navya'): 1\n",
            "('Navya', 'I'): 1\n",
            "('am', 'pleased'): 1\n",
            "('pleased', 'to'): 1\n",
            "('to', 'meet'): 1\n",
            "('meet', 'you'): 1\n",
            "('you', '.'): 1\n",
            "('.', 'Hope'): 1\n",
            "('Hope', 'you'): 1\n",
            "('you', 'are'): 1\n",
            "('are', 'doing'): 1\n",
            "('doing', 'well'): 1\n",
            "('well', '.'): 1\n",
            "\n",
            "Bigram with Highest Frequency: ('I', 'am') (2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzdkCS9jYv_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}