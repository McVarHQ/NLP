{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Use vector space model and cosine similarity for text classification."
      ],
      "metadata": {
        "id": "drtNZHgwQxhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnD6_QogaRTZ",
        "outputId": "6ffe03a4-d231-4ca3-9d7a-75fb82a7455b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/717.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from odfpy) (0.7.1)\n",
            "Building wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160671 sha256=575742679f3f0f2fb0eb71e1eedffa10a682779a0074ae28ccc43d65243c7110\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/2e/95/90d94fe33903786937f3b8c33dd88807f792359c6424b40469\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy\n",
            "Successfully installed odfpy-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('ChinaJapan.ods', engine='odf')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEuo4-IYZnX4",
        "outputId": "604260dd-ec38-4d6a-f161-653daadb415d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Doc                    Words  Class\n",
            "0    1   Chinese Beijing Chinese     c\n",
            "1    2  Chinese Chinese Shanghai     c\n",
            "2    3             Chinese Macao     c\n",
            "3    4       Tokyo Japan Chinese     j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a.Construct the vector space model (Preprocess the text, calculate Bag of words and TF-IDF) and compute the importance of the word Chinese in the test data."
      ],
      "metadata": {
        "id": "6oNDpZPWZYII"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOHa1MIpOHgC",
        "outputId": "27fa6c5c-17ea-4bc0-dcfa-42b4c4d5b06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('ChinaJapan.ods', engine='odf')\n",
        "print(df.head())\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "docs = []\n",
        "for i in range(len(df)):\n",
        "\n",
        "  doc = df.iloc[i]['Words ']\n",
        "  tokens = word_tokenize(doc)\n",
        "  tokens = [t.lower() for t in tokens if t.isalnum()]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [t for t in tokens if t not in stop_words]\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [stemmer.stem(t) for t in tokens]\n",
        "  docs.append(tokens)\n",
        "\n",
        "df['Processed'] = docs\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM6mKf-Ud7d-",
        "outputId": "87391db3-b19f-47c6-e04a-a02f0c7486d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Doc                    Words  Class\n",
            "0    1   Chinese Beijing Chinese     c\n",
            "1    2  Chinese Chinese Shanghai     c\n",
            "2    3             Chinese Macao     c\n",
            "3    4       Tokyo Japan Chinese     j\n",
            "Index(['Doc', 'Words ', 'Class'], dtype='object')\n",
            "   Doc                    Words  Class                   Processed\n",
            "0    1   Chinese Beijing Chinese     c      [chines, beij, chines]\n",
            "1    2  Chinese Chinese Shanghai     c  [chines, chines, shanghai]\n",
            "2    3             Chinese Macao     c             [chines, macao]\n",
            "3    4       Tokyo Japan Chinese     j      [tokyo, japan, chines]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(df['Processed'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "bow_df = pd.DataFrame(bow.toarray(), columns=feature_names)\n",
        "\n",
        "\n",
        "print(bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rMzlWGYeNqj",
        "outputId": "7699a705-abfb-42fa-9146-f2adcd3da41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   beij  chines  japan  macao  shanghai  tokyo\n",
            "0     1       2      0      0         0      0\n",
            "1     0       2      0      0         1      0\n",
            "2     0       1      0      1         0      0\n",
            "3     0       1      1      0         0      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the processed documents\n",
        "tfidf = tfidf_vectorizer.fit_transform(df['Processed'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "# Get the feature names (unique words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf.toarray(), columns=feature_names)\n",
        "\n",
        "# Display the TF-IDF representation\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJcJGsBHhC9q",
        "outputId": "43f7dc37-6b0b-463d-9370-4f8292bc757e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       beij    chines     japan     macao  shanghai     tokyo\n",
            "0  0.691835  0.722056  0.000000  0.000000  0.000000  0.000000\n",
            "1  0.000000  0.722056  0.000000  0.000000  0.691835  0.000000\n",
            "2  0.000000  0.462637  0.000000  0.886548  0.000000  0.000000\n",
            "3  0.000000  0.346182  0.663385  0.000000  0.000000  0.663385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = input(\"Enter test data: \")\n",
        "\n",
        "test_tokens = word_tokenize(test_text.lower())\n",
        "test_tokens = [t for t in test_tokens if t.isalnum() and t not in stop_words]\n",
        "test_tokens = [stemmer.stem(t) for t in test_tokens]\n",
        "\n",
        "test_tfidf = tfidf_vectorizer.transform([' '.join(test_tokens)])\n",
        "\n",
        "try:\n",
        "  chinese_index = np.where(feature_names == 'chines')[0][0]\n",
        "  importance = test_tfidf[0, chinese_index]\n",
        "  print(\"Importance of 'Chinese' in test data:\", importance)\n",
        "except IndexError:\n",
        "  print(\"The word 'Chinese' (or its stemmed form) is not present in the test data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cuygW1lhUNB",
        "outputId": "8bb8930c-7c70-4129-d7a2-778ddd120f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter test data: Chinese Chinese Chinese Tokyo Japan\n",
            "Importance of 'Chinese' in test data: 0.7420574954436144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###b.Find the similarity of the test data considering with any one document from training data using the cosine similarity evaluation metric"
      ],
      "metadata": {
        "id": "CF2dd5hLiwMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "doc_index = int(input(\"Enter the index of the document from training data to compare with (0 to {}): \".format(len(df)-1)))\n",
        "\n",
        "similarity = cosine_similarity(test_tfidf, tfidf[doc_index])\n",
        "print(\"Cosine similarity with document {}: {}\".format(doc_index, similarity[0][0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Mxq4iWhinD",
        "outputId": "9c268ca3-00bd-43be-dbd7-0f016fb28887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the index of the document from training data to compare with (0 to 3): 2\n",
            "Cosine similarity with document 2: 0.34330349920760334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###c.Take a dataset of your own labelled with sentiment. Split the training and testing part and compute the sentiment classification with the application of Laplace smoothing"
      ],
      "metadata": {
        "id": "tFNqgE1Sjcax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('SA.ods', engine='odf')\n",
        "print(df1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBGLkO31jCq0",
        "outputId": "42c0674e-16b0-47ce-e125-805cacf99692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No.                                          Text  Class\n",
            "0    1      I had a wonderful time at the park today      1\n",
            "1    2                     This new phone is amazing      1\n",
            "2    3             The movie I watched was very bad       0\n",
            "3    4  I love my new job, the team is so supportive      1\n",
            "4    5                     I had a bad day at work.       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "X = df1['Text']\n",
        "y = df1['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "X_train_processed = X_train.apply(preprocess_text)\n",
        "X_test_processed = X_test.apply(preprocess_text)\n",
        "\n",
        "# TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_processed)\n",
        "X_test_tfidf = vectorizer.transform(X_test_processed)\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB(alpha=1.0)\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# User input for classification\n",
        "user_input = input(\"Enter text to classify sentiment: \")\n",
        "user_input_tokens = word_tokenize(user_input.lower())\n",
        "user_input_tokens = [t for t in user_input_tokens if t.isalnum() and t not in stop_words]\n",
        "user_input_tokens = [stemmer.stem(t) for t in user_input_tokens]\n",
        "user_input_tfidf = vectorizer.transform([' '.join(user_input_tokens)])\n",
        "\n",
        "predicted_class = nb_classifier.predict(user_input_tfidf)[0]\n",
        "\n",
        "if predicted_class == 1:\n",
        "  print(\"Positive\")\n",
        "elif predicted_class == 0:\n",
        "  print(\"Negative\")\n"
      ],
      "metadata": {
        "id": "OIqYPGkrnLE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492c1108-4078-4934-a494-2b96e8a01e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Enter text to classify sentiment: The interviewer was rude, but I did well\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5UHTWUALFoN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}